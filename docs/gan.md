# Generative Adversarial Network
## GANs Overview:

**GAN is an unsupervised deep learning algorithm where we have a Generator pitted against an adversarial network called Discriminator.**

**Generator generates counterfeit currency. Discriminators are a team of cops trying to detect the counterfeit currency. Counterfeiters and cops both are trying to beat each other at their game.**

![GAN Image](https://miro.medium.com/v2/resize:fit:828/format:webp/1*t78gwhhw-hn1CgXc1K89wA.png)

<p align="center"><small>Image source: <a href="https://medium.com/@medium/your-article-link">Medium</a></small></p>

Generatorâ€™s objective will be to generate data that is very similar to the training data. Data generated from Generator should be indistinguishable from the real data.

Discriminator takes two sets of input, one input comes from the training dataset(real data) and the other input is the dataset generated by Generator.

### 1) Generator:
The generator's main role is to create realistic data samples that are similar to the training data. It takes random noise as input and gradually transforms it into data samples that should resemble real data. The generator's architecture is typically a deep neural network, often constructed using layers like fully connected (dense) layers or convolutional layers in the case of images. The generator starts with random noise and progressively refines it through multiple layers to generate more complex and coherent samples.


### 2) Discriminator:
The discriminator's primary task is to distinguish between real data samples from the training dataset and fake samples generated by the generator. Like the generator, the discriminator is also a deep neural network. It takes input data (either real or fake) and produces a probability score indicating whether the input is real or fake. The discriminator's architecture is similar to that of a binary classifier, aiming to learn to differentiate between real and fake samples effectively.


### 3) <ins>Training Process:</ins>
- Generator Training: Initially, the generator produces fake data samples using random noise as input. These fake samples are then passed to the discriminator for evaluation.


- Discriminator Training: The discriminator receives both real data samples from the training set and fake samples from the generator. It is trained to correctly classify real samples as real (label 1) and fake samples as fake (label 0).

### 4) <ins> Loss Functions:</ins>
- **Generator Loss:** The generator's objective is to create samples that are convincing enough to fool the discriminator. To achieve this, the generator aims to minimize the difference between the discriminator's predictions on fake samples and the desired label (1, indicating real). In other words, the generator's loss function encourages it to produce samples that the discriminator thinks are real.


- **Discriminator Loss:** The discriminator's goal is to accurately classify real and fake samples. Its loss function involves a combination of the errors made on real and fake samples. The goal is to maximize the difference between its predictions on real and fake samples.

### 5) Minimax Game:

The training process can be understood as a minimax game where the generator and discriminator are engaged in adversarial training. The generator updates its parameters to minimize its loss while the discriminator updates its parameters to maximize its accuracy in distinguishing real from fake samples. This process leads to a dynamic equilibrium where the generator produces more realistic samples as the discriminator becomes better at its task.


### 6) GAN Loss Functions

In a Generative Adversarial Network (GAN), there are two primary loss functions: one for the generator and another for the discriminator.

**Generator Loss (L<sub>G</sub>)**

The goal of the generator is to produce data samples that are indistinguishable from real data. The generator loss is calculated based on the discriminator's classification of the generated samples. It can be expressed as the negative log likelihood of the discriminator being mistaken about the generated samples:

![Generator Loss](https://latex.codecogs.com/svg.image?L_G&space;=&space;-\log(D(G(z))))

Where:
- **D(G(z))** is the discriminator's output when evaluating the generator's output, given random noise **z** as input.


**Discriminator Loss (L<sub>D</sub>)**

The discriminator's role is to differentiate between real and generated samples. Its loss is composed of two terms: one corresponding to the classification error for real samples and the other for fake samples. The discriminator aims to maximize its accuracy in distinguishing real from fake samples.

![Discriminator Loss](https://latex.codecogs.com/svg.image?L_D&space;=&space;-\log(D(x))&space;-&space;\log(1&space;-&space;D(G(z))))

Where:
- **D(x)** is the discriminator's output when evaluating a real data sample **x**.
- **D(G(z))** is the discriminator's output when evaluating the generator's output, given random noise **z** as input.

 > #### L = log(sigmoid(D(x))) + log(1-sigmoid(D(G(z))))

These loss functions create an adversarial training process, where the generator aims to minimize its loss by generating samples that the discriminator considers real, while the discriminator aims to maximize its loss by correctly classifying real and generated samples.

### 7) Nash Equilibrium

GAN is based on the zero-sum non-cooperative game. In short, if one wins the other loses. A zero-sum game is also called minimax. Your opponent wants to maximize its actions and your actions are to minimize them. In game theory, the GAN model converges when the discriminator and the generator reach a Nash equilibrium. This is the optimal point for the minimax equation below.

![nash1](https://miro.medium.com/v2/resize:fit:828/format:webp/1*l9se1koH_eQdZesko5eQpw.jpeg)

The Nash equilibrium refers to a scenario in which there exists no motivation for players to stray from their initial strategy alone. Consider two player A and B which control the value of x and y respectively. Player A wants to maximize the value xy while B wants to minimize it.

![nash2](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FT0yTKZhl1JsxR6uN026w.jpeg)

The Nash equilibrium is x=y=0. This is the state where the change of mind of a single player will not improve the result. 

### 8) Advantages of GANs over single-cell data generation:
- **Diversity:** GANs can generate diverse and realistic single-cell data that encompass a wide range of cellular states and phenotypes, overcoming the limitations of sample heterogeneity in real datasets.
- **Scalability:** GANs can scale to generate large quantities of synthetic single-cell data, mitigating the constraints imposed by limited sample availability.
- **Cost-effectiveness:** By reducing the dependency on expensive experimental procedures and sample acquisition, GAN-based approaches offer a cost-effective alternative for generating single-cell data.
- **Privacy Preservation:** GANs enable the generation of synthetic data that preserve the privacy of sensitive patient information, addressing ethical concerns related to data sharing and patient consent.

### 9) Advantages of GANs over other types of generative models:
- **Implicit Modeling:** GANs don't explicitly model the probability distribution of the data, unlike Boltzmann Machines (RBMs), Deep Belief Networks (DBNs), Deep Boltzmann Machines (DBMs), and Generative Stochastic Networks (GSNs). This allows GANs to capture complex and high-dimensional data distributions without making assumptions about their structure.

- **No Markov Chains:** Unlike RBMs and DBNs, GANs do not rely on Markov chains for sampling. This leads to faster convergence during training and more efficient generation of samples.

- **No Layer-by-Layer Training:** GANs do not require layer-by-layer pre-training like DBNs and DBMs. This simplifies the training process and reduces the risk of getting stuck in local optima.

- **No Reconstruction Loss:** Unlike Denoising Autoencoders (DAEs), GANs do not rely on a reconstruction loss function. Instead, they learn to generate samples by directly optimizing a discriminator network to distinguish between real and fake data.

- **Better Sample Quality:** GANs often produce higher quality samples compared to other generative models, as evidenced by their success in generating realistic images, audio, and text data.
### 10) Disadvantages of GANs:
Generative Adversarial Networks (GANs) have been a revolutionary advancement in machine learning and deep learning, particularly in the field of generative models. However, despite their impressive capabilities, GANs also come with several disadvantages:

**i. Training Instability:**

- GANs are notoriously difficult to train due to the delicate balance required between the generator and discriminator. If one model becomes too strong compared to the other, the training can fail.
- Mode collapse, where the generator produces limited variety in its outputs, is a common issue.
  
**ii. Resource Intensive:**
  
- Training GANs often requires significant computational resources, including powerful GPUs and extensive memory, due to the complexity and size of the networks involved.
- The iterative nature of training two competing networks (generator and discriminator) can be time-consuming.
  
**iii.  Sensitivity to Hyperparameters:**

- GANs require careful tuning of hyperparameters (such as learning rates, batch sizes, etc.), and small changes can significantly impact the training outcome.
- Finding the right architecture and hyperparameters for both the generator and discriminator can be a trial-and-error process.
  
**iv.  Lack of Evaluation Metrics:**

- Evaluating the performance of GANs is challenging because there are no definitive metrics to measure the quality and diversity of generated outputs. Common metrics like Inception Score (IS) and Frechet Inception Distance (FID) have limitations and can be misleading.
  
**v.  Mode Collapse:**

- Mode collapse is a phenomenon where the generator fails to capture the full diversity of the data distribution and produces only a few types of outputs repeatedly.
- This leads to a lack of variability in the generated data, reducing the effectiveness of the GAN.
  
**vi.  Gradient Vanishing and Exploding:**

- The training of GANs can suffer from issues where gradients either vanish (become very small) or explode (become


**vii. Applications:**

- **Image Synthesis:**
  - GANs have been widely used to generate realistic images, including faces, objects, and scenes.
Applications range from creating high-resolution artwork to generating diverse datasets for training machine learning models.

- **Style Transfer:**
  - GANs can transfer artistic styles from one image to another, allowing for creative transformations.
This application is often used in the fields of art and design for producing unique visual outputs

- **Image-to-Image Translation:**
  - GANs enable the transformation of images from one domain to another, such as turning satellite images into maps or converting black-and-white photos to color.
This has applications in augmented reality, image enhancement, and domain adaptation.

- **Data Augmentation:**
  - GANs can generate synthetic data to augment training datasets, improving the robustness and generalization of machine learning models.
This is particularly useful in scenarios where obtaining large labeled datasets is challenging.

## GAN implementation in GAT-GAN:
Generative Adversarial Networks or GANs which belongs to the family of generative models, emerge as a promising solution to address these challenges by synthesizing realistic cell samples that mimic the complexity and diversity of real biological samples. Synthetic data generation using GANs presents opportunities to navigate the ethical challenges, such as patient privacy, informed consent, and the equitable distribution of research benefits, while advancing research objectives. By generating cell samples that do not directly match to individual cell patient samples, researchers can protect patient privacy and
minimizing the risks of data misuse. However, it is very much important to ensure that synthetic data accurately reflect the underlying biological processes and do not introduce biases or artifacts that could
compromise research validity.
After subsampling the data and with the priority nodes generate the realistic synthetic cell samples that easily mimic the real cell samples. On the later point of time with the different clustering techniques(UMAP Clustering) and different metric we will satisfy that the generated data is fare enough.

---
